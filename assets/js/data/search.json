[ { "title": "Essentials for each Repository", "url": "/posts/essentials-for-each-repository/", "categories": "Software-Development, Tools", "tags": "Soft-Skills", "date": "2023-04-15 01:00:00 +0200", "snippet": "Version control is one of the great skills that a software developer should have. Though there is a lot to learn, the basics are readily available on the internet to learn. Commands are well documented and there are multiple software options for version control readily available.Another important part of version control is the structure and information provided with whatever you version control. The repository structure makes your work easily navigable. Basically it comes down to working in pure chaos is difficult, thus structure your work to make it easier. The structure you want will depend mostly on your work. As an example, Python code files will be structured differently from Java code files.A second part would then be keeping the structure clean. This will partly be a manual process, but it is possible to prevent unnecessary clutter into your repository. A gitignore file (.gitignore) will prevent files and directories from being committed in the repository. The webtool gitignore.io helps in creating a .gitignore file for most programming languages to keep unnecessary files out of your repository.# python compiled code*.pyc# rubygem*.gem# VScode directory with specific settings.vscode/Besides the structure, there are two files that are important for a repository. These are the README and the the license. The README is the introduction for your repository, the poster-child if you will. Sites such as GitHub uses the README as the default file to display. Thus the README is a great place for some basic explanation about your project and how to use it. A good place to start writing a README is readme.so.The license on the other hand is a file that tells others what they are allowed to do with your work. Only allowing to use the project as-is will require a different license from if you want people to freely share it. I’m not a legal advisor, so I cannot recommend one license over another. A great resource for the pros and cons of different licenses is choosealicense" }, { "title": "Scheduling Tasks on Linux with CRON", "url": "/posts/scheduling-tasks-on-linux-with-cron/", "categories": "Software-Development, Tools", "tags": "Linux", "date": "2023-04-01 01:00:00 +0200", "snippet": "We have the ability to create processes to simplify our tasks immensely as programmers. Able to turn whole paragraphs of text (a script) into a few words (a command) gives great flexibility, efficiency, and productivity. However, some tasks need to be repeated quite often, say every day or even every hour. Coming to your terminal and typing the same command every time gets tedious real fast.On unix devices (thus also servers) we can use cron to run tasks on a fixed schedule. cron is a commandline utility with which we can specify the moment a script or command should be executed. As long as the computer in question is on, it will execute the scheduled job.To set a job, we will need to at it to a specific file. This file can be opened using the command crontab -e. If the file does not exist, it will create one for you and open it in the editor of your choice. In this file you can add jobs with the format: Minute Hour DayOfMonth Month DayOfWeek Command. There are shorthands starting with an @ symbol such as @yearly. The command can be a standard unix command such as ls or the start of script like bash run_script.sh. A few examples are given below.15 * * * * command # Every 15 minutes0 4 * * sun command # Every Sunday at 4 AM0 9 8-14 * * command # Every 8th till 14th of the month at 9 AM@yearly command # Every yearThere are many ways to set the scheduling. A useful website for cron is crontab.guru. This gives an interactive interface to play with the time settings. The great part is that crontab.guru gives an understandable explanation of the given time settings back as feedback. It also has a short list of the options for the time settings. There are many ways to extend the scheduling to your own custom uses. Both this post and crontab.guru are starting points to automate and schedule some processes to simplify your life. The best way to learn it is by trying different settings yourself." }, { "title": "Generating Unique Identifiers in Python", "url": "/posts/generating-unique-identifiers-in-python/", "categories": "Software-Development, Programming", "tags": "Python", "date": "2023-03-18 00:00:00 +0100", "snippet": "Automating database processes are easy to do in Python. There are multiple packages allowing for interaction with different kinds of databases such as SQLalchemy. Thus we can place information from our other Python scripts directly into the appropriate databases through Python itself.A common task is generating new unique identifiers for items in the databases. It is good practice to have an unique identifier that is separate from all other kind of data to ensure privacy. Of course it is possible to make unique identifiers by combining several pieces of information. However can it be guaranteed that those combinations will always be unique? What will be the effect if one of the subfields of information changes? Using a combination of information can lead to a whole can of worms of unforeseen issues.Thus Universal Unique IDentifier’s (UUIDs) are generally preferred for truly unique identifiers. They are also called General Unique IDentifier’s (GUIDs). Python has a standard library (since Python 2.5) for generating UUID. This library provides four methods for generating UUID according to slightly different approaches.import uuidunique_id_1 = uuid.uuid1()unique_id_3 = uuid.uuid3(unique_id_1, &quot;some_string&quot;)unique_id_4 = uuid.uuid4()unique_id_5 = uuid.uuid5(unique_id_1, &quot;some_string&quot;)The first method is uuid1(). This makes an identifier from the host ID, a sequence number, and the current time. The host ID comes basically from the device you are using. The sequence number is a random sequence number of 14 bits. Due to the host ID, it is technically possible to trace down by who the identifier is generated, which can be good or bad depending on your own requirements.The second method, named uuid3() (not 2), generates the UUID quite differently. For uuid3() you need to give a UUID and a string. Both are hashed together using the MD5 hash, resulting in a UUID. The string can be a standard string of your own choosing. Thus this method gives a bit more control on the resulting UUID as you can always supply the same values, which is great for reproducible and predictable UUIDs. The same goes for the method uuid5(), but uses SHA1 hash instead of the MD5 hash for generating the UUIDs.Finally, uuid4() gives a completely random identifier. This is likely the best method if we want to use the UUID for anonymity and privacy concerns. If there are any concerns with the UUIDs you can always check if the generated UUID is already in the database, though I doubt that is an issue as long as the UUIDs are generated through the same method." }, { "title": "Negative Selection in R", "url": "/posts/negative-selection-in-r/", "categories": "Software-Development, Programming", "tags": "R", "date": "2023-03-04 00:00:00 +0100", "snippet": "R is a statistical programming language well suited for data wrangling and analysis. For many data preparation steps we need to remove data columns. In many languages we achieve this by selecting the columns we want to keep, or using a specific functions such .drop() from Python’s pandas package.With R we can also achieve this using negative selection. Similar to the .drop() method we can specify the values, columns, or rows that we want to remove. In the case of vectors we use the square bracket notation and place a minus sign before the index we want removed. If want multiple indices removed we place the minus sign before the vector with the indices.var &amp;lt;- c(&quot;a&quot;, 2, &quot;words&quot;, 123.964)print(var[-3]) # [1] &quot;a&quot; &quot;2&quot; &quot;123.964&quot;print(var[-c(3, 4)]) # [1] &quot;a&quot; &quot;2&quot;For dataframes indices indicate rows and columns. Thus in a similar fashion as above we can remove the unwanted data. With the bracket notation Be aware that with negative selection you will remove the entire row and column if both are given, not just a specific value in the dataframe.name &amp;lt;- c(&quot;Jon&quot;, &quot;Bill&quot;, &quot;Maria&quot;, &quot;Ben&quot;, &quot;Tina&quot;)sex &amp;lt;- c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;)age &amp;lt;- c(23, 41, 32, 58, 26)df &amp;lt;- data.frame(name, sex, age)print(df[,-1])# sex age# 1 Male 23# 2 Male 41# 3 Female 32# 4 Male 58# 5 Female 26The same can be achieved through the dplyr package, which is part of the tidyverse. This allows us to easily use negative selections with the names of the columns instead of indices. For negative selection to work properly with dplyr the names need to be wrapped in a vector as shown in the example below.library(dplyr)name &amp;lt;- c(&quot;Jon&quot;, &quot;Bill&quot;, &quot;Maria&quot;, &quot;Ben&quot;, &quot;Tina&quot;)sex &amp;lt;- c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;)age &amp;lt;- c(23, 41, 32, 58, 26)df &amp;lt;- data.frame(name, sex, age)print(df %&amp;gt;% select(-c(name)))# sex age# 1 Male 23# 2 Male 41# 3 Female 32# 4 Male 58# 5 Female 26Through negative selection it is possible to reduce the amount of code needed. Simply by using the negative instead of the positive. Negative selection is an easy but powerful tool. It is a great way to make your code more readable, and also save yourself unnecessary typing." }, { "title": "A Basic NGS Data Analysis Workflow", "url": "/posts/a-basic-ngs-data-analysis-workflow/", "categories": "Bioinformatics", "tags": "Bioinformatics", "date": "2023-02-18 00:00:00 +0100", "snippet": "Next Generation Sequencing (NGS) data has slowly become the standard in bioinformatics, replacing PCR and microarray expression data. Though all three hold genetic information, NGS data is unique in that it directly gives the genetic sequences of a sample. Usually these sequences are stored in FASTQ formatted files. To analyze these files into something useful we develop software/computational pipelines.A common first step in a pipeline is a quality control (QC) step with a tool such as FastQC. This is to ensure that the input, the FASTQ files are good. As they say, garbage in will only give garbage out. Thus it is important to measure the quality of the data. If you want to improve the pipeline further, you could implement a switch stopping the pipeline if the quality of the data is not high enough. Otherwise you can flag the data after it has been analyzed.Once you know the quality of the input data it is time to prepare it for use. Almost all NGS data comes from Illumina sequencers, which uses specific short sequences bound to the actual sequences from the sample. These short sequences are called adapters and should be removed before further processing. The tools Cutadapt and Trimmomatic are commonly used for this purpose. Both tools can also be used for sequence trimming. This will remove nucleotide bases from the beginning and end of the sequence that are below par, though this will require some fine-tuning. It can be useful to look at quality of the data again at this point.When you know that the data is prepped and ready we can go on the next step. With this step we align the sample sequences to a reference. This is achieved using aligner programs such as STAR, Bowtie2, and BWA. Each uses the reference as a template to match the sample sequences against to correctly place them to their corresponding genes (and other genetic locations). The reference is the entire genome that is comparable to the sample. So if you have human sample, use a human reference. It is recommended to use the latest reference available.After creating the alignment, the follow-up steps will depend on the goal you want to achieve and the kind of NGS data you have. NGS data can largely be divided into DNA and RNA sequenced samples. With RNA sequenced data you measure the transcriptome, thus the activation of genes at a specific moment in time. DNA sequenced data on the other hand is more stable over time and describes the entire genetic landscape better.The difference between DNA and RNA data can limit the kind of analysis you are able to perform. Each analysis has multiple steps that deserves its own blog post. So in short; An expression analysis can give us the genes that are activated, and by reverse inference thus also the ones that are deactivated. Of course this is limited to RNA data. A genetic association study on the other hand requires the stable DNA of many samples to find structural differences in the genome between populations. Variant calling, to uncover structural differences of one sample against a reference, can be done with both DNA and RNA data. Though DNA data usually gives higher quality results.Depending on the data you have and the goals you want to achieve it will be necessary to adjust and refine the pipeline. My advise would be ITERATE. Start with the commands in the shell and fine-tune them to your liking. Iterate on the code and put it in a script to automate it, this can be any kind of script though shell scripts are advised. Further iterate by searching for processes that could be run in parallel or on multiple cores to make maximum use of the available hardware. It can be useful to use workflow software such as Snakemake or NextFlow at this point. Hopefully this post gives some guidance and helpful tips to start developing your NGS analysis pipeline." }, { "title": "Making Aliases", "url": "/posts/making-aliases/", "categories": "Software-Development, Tools", "tags": "Linux, BASH", "date": "2023-02-04 00:00:00 +0100", "snippet": "Many commands in the linux terminal environment are highly customizable using flags and options. Often we use the same commands with the same settings. Aliases can be used to set the flags and options by default, or name new commands with these behaviors. Thus instead of typing ls -a -l -t or the shorter form ls -alt, we can create an alias lt that does the same. I basically use aliases to have human readable format enabled by default for commands such as ls and df.An alias can be created in the terminal using the alias command as seen below. The basic syntax is alias &amp;lt;name&amp;gt;=&quot;&amp;lt;command&amp;gt;&quot;. Any valid command can be put in there, even with pipes (|) chaining multiple commands into one.alias lt=&quot;ls -alt&quot;However, simply executing this command will only allow you to have this command for the current session. To have these aliases available to you every time we will put them in the shell startup configuration script, which is run each time when opening a terminal. In the case of the BASH shell it is .bashrc, for the Oh-My-Zsh shell it is the file .oh-my-zsh. The standard Zsh shell uses .zshrc.Though it is possible to put the aliases almost anywhere in the file, it is recommended to put them at the end. This is to prevent anything breaking during setup of the terminal. Similarly, be careful of the order of the aliases. If a new alias uses an existing one, it must come after it in the configuration script.When you create a lot of aliases it might be better to create a separate file for them. This can basically be any file. It is recommended to have this file in your home directory and be hidden (starting with a .). In your shell configuration script you put the following code. Replace &amp;lt; &amp;gt; with your own filename. This will call the file with your aliases and thus loading them into your terminal environment.if [ -e $HOME/&amp;lt;.bash_aliases&amp;gt; ]; then source $HOME/&amp;lt;.bash_aliases&amp;gt;fiWith this you can start making your own aliases. This can help with automating some simple commands, making the defaults useable and sensible. You could also automate your ssh login, though be careful with passwords. In short you can use aliases to make your development processes nicer and suited for your needs." }, { "title": "Showing Significance in R Plots", "url": "/posts/showing-significance-in-r-plots/", "categories": "Software-Development, Programming", "tags": "R", "date": "2023-01-21 00:00:00 +0100", "snippet": "R is an amazing programming language for statistical analysis. With the extension of the ggplot2 package it is relative easy to generate meaningful plots. These plots are great for visualizing and describing the data. However, ggplot2 is limited in providing annotations for significance. While obvious differences do not necessarily need significance to prove their distinction, there are many cases where such annotations would be beneficial.Significance can show statistical differences, even though they seem to be small and irrelevant. Though this does indicate that the difference is meaningful, it can be a pointer to an interesting find. Though calculating the significance is a separate topic of itself, showing the significance in the plots makes it easier to use in presentations, papers, and other publications. The package ggsignif allows us to add significance annotations to ggplot2 figures. An example using the classic “auto mpg” dataset is given shown below.Within the code, we add the significance annotation with geom_signif(). We specify the combinations of categories that we want to compare for significance by providing a list of vectors. Each vector is a pair of strings, with the strings being the labels/factors used to generate the plot. Note that only the first two strings are used if more than two strings are given. The list is given as argument to the parameter comparisons. This will show the numeric significance above brackets, with the brackets indicating the comparisons.library(ggplot2)library(ggsignif)p1 &amp;lt;- ggplot(mpg, aes(class, hwy)) + geom_boxplot() + ylim(NA, 48) + geom_signif(comparisons=list(c(&quot;compact&quot;, &quot;midsize&quot;), c(&quot;minivan&quot;, &quot;suv&quot;)))Of course, there are more options to play with. A simple change is the use of symbols instead of numeric values. This can be easily achieved using the map_signif_level parameter within geom_signif(). Setting the parameter to TRUE gives the default mapping, “***” for 0.001, “**” for 0.01, and “*” for 0.05. Any higher values are determined to non-significant (NS.) by the default mapping. You can also give your own mapping in the form of a list.Another simple change is turning the plot on its side. This is normally done with coord_flip() from the ggplot2 package. Luckily this works out of the box with ggsignif, thus it is incredibly easy to do make this change. The resulting plot can be seen below.library(ggplot2)library(ggsignif)p2 &amp;lt;- ggplot(mpg, aes(class, hwy)) + geom_boxplot() + ylim(NA, 48) + geom_signif(comparisons=list(c(&quot;compact&quot;, &quot;midsize&quot;), c(&quot;minivan&quot;, &quot;suv&quot;)), map_signif_level=T) + coord_flip()There are even more options available to customize the significance annotations with ggsignif. For those that are interested I recommend reading the documentation. With the basics covered here we can at least start improving our plots with significance." }, { "title": "Tools are just that... Tools", "url": "/posts/tools-are-just-that-tools/", "categories": "Personal", "tags": "Soft-Skills, Other", "date": "2023-01-07 00:00:00 +0100", "snippet": "While working and learning I noticed a peculiarity in discussions, tutorials, and general advice. A strange focus on the the tools instead of the problem at hand is usually there. This is mostly because people, myself included, like to use what we already know and are familiar with. This goes for programming language, development methodology, and other tools.Though there is nothing wrong with relying on the tools you know, an over-reliance in never great to have. By fitting the problem to the tools there will come a point that the “solution” will not work, or work but is not adaptable. Simply by changing the problem for our tools to work we can create countless more problems for ourselves. Even more so because sometimes we do not even solve the problem at hand.Beware that this issue is not only limited to direct tools such as a programming language or design software. Realize that most. As an example. in the last few years there has been an increased focus on Test Driven Development (TDD) as THE method for developing software. While TDD is great and has many advantages, some have become dogmatic in its use and forget that it is a tool, not a natural law. There are cases where TDD will not work as well, such as biological research where coding is only used for the analysis.So while we can use TDD everywhere for software development, it is not always the best solution. For a quick biological analysis it will generate a lot of bloat and unnecessary specifications (entire framework pipeline) that will not be used, looked at, and be relevant for the developers and users. To re-iterate again, this is not only TDD, or programming languages. This dogmatic focus can be placed on any tool. Try to find the ones that work for the problem, and learn them to solve it.With any real world problem there are without doubt multiple ways to solve it. So in the end, does it really matter that we focus more on the tools than the problem? Likely not, but I do believe it is good to keep in mind that tools are simply tools. If warranted, we can fit the problem as close as possible to our own skills and tools without truly distorting our problem at hand. Otherwise, we just have to learn more tools to solve our problems, which should already be part of the job." }, { "title": "Dumping Print to Files in R using Sink", "url": "/posts/dumping-print-to-files-in-r-using-sink/", "categories": "Software-Development, Programming", "tags": "R", "date": "2022-12-24 00:00:00 +0100", "snippet": "Writing data to files is an essential step in all data analysis. To save your mutated data, to make the data available for other, or to simply view the data, all of these are valid reasons. R gives plenty of options to achieve this, such as the write.table and write.csv functions. However, data needs to be properly formatted to get clean data out of it.A simple example of this issue is the output from the summary function. It gives a great overview of your data in some basic statistics. But it only looks great when printing the results to the screen. When writing to a file it always has some strange undesired artifacts. These kind of issues only occur more often when you have specific packages for rare use-cases.So writing to a file is not optimal, but with just printing we miss the entire point of saving the data. Luckily there is the sink function. This allows for writing everything to a file instead of the screen. An example of using sink can be seen below. Note that we call sink again to close the process, otherwise everything will continue to be written to the file until an error occurs or you quit R.sink(&quot;output.txt&quot;)print(some_variable)sink()As in the example, we specify an output file to write the data towards. I recommend to initially always have it be a simple text file (.txt). With this format the data is always viewable. Once you have checked and are sure that another format can be used, simply change the extension to the format of choice and re-run the program. In this manner sink will be a valuable tool for you to use in debugging and even for getting strange data in simple text file, which is always handy to be able to do." }, { "title": "Priority Stacks: A Time Management System", "url": "/posts/priority-stacks-a-time-management-system/", "categories": "Personal", "tags": "Other", "date": "2022-12-10 00:00:00 +0100", "snippet": "There is always a limit to time, and we cannot buy easily more of it. Thus we try to efficiently manage our time. By efficiently making use of our time we can try to reduce stress and ensure we do what we want to do. Time management systems on both the small and large scale such as the Pomodoro technique and agile sprints are well known examples.I developed my personal system that works for me. In all honesty, it is likely pieced together from the advices I’ve gotten from many different people. Simply put, it is a task dividing method. Divide your tasks based on priority and then rank them based on time. By ordering on time needed, you can pick and choose your tasks to whatever your schedule is. A simple division would be as follows.1) Recurrent; tasks that occur periodically such as preparation for weekly meetings.2) Deadline; tasks that need to be finished by a specific date and time. The work your manager gives you and preferably wanted done yesterday.3) Important; tasks that need to be done but without any further pressure. Updating documentation is a great example of this.4) Inconsequential; tasks that could be helpful, but are of little note. Little things as checking a mail-list for information or reading a scientific paper are examples as long as they are not your main job.It is of course possible to make fewer or more stacks though I recommend at least 3 stacks and a maximum of 7. Try to keep it manageable and organized. For the same reasons, try to have a maximum on the number of tasks within each stack. This can also aid in preventing overworking, and in the worst-case burnout. Play around to see what works well for you.There are other options to try out as well. Place your tasks into your calender. This is easy for the recurrent tasks, but also try to schedule time specific for different kinds of tasks. Don’t forget, variety is the spice of life. Another option would be to combine the stacks with a kanban-style board, moving the tasks from left to right to even track the ongoing status of them.In short, my personal stacks system can be a good basis and has enough room to make it your own. Try it out, see if it works for you. Throw away the things that do not work and keep what does. Just use a system to help yourself, because it will only be better if people can manage their time and tasks in a way that works great for them." }, { "title": "Python f-Strings are Magic", "url": "/posts/python-fstrings-are-magic/", "categories": "Software-Development, Programming", "tags": "Python", "date": "2022-11-26 00:00:00 +0100", "snippet": "Formatting strings with variables is a common task in programs. Through the years, different methods have become available to achieve this in Python. The easiest, but also least efficient, method is by concatenating the strings and variables. For this to work the variables need to be strings, so it is recommended to convert all variables into strings with the str() function.A second method is the old-style format method. This uses the % symbol for the places where a variable should come. The variables are given in order within a tuple (an immutable list). Formatting is done through indicators after the %. In this case variables are converted to strings due to the s.The new-style format method is styled in more ‘pythonic’ manner. Instead of the %, curly brackets {} are used to denote the placing of the variables. Then .format() is called with all the needed variables. Within the brackets one can place integers to denote a specific variable. If both brackets contain the value 1, then only the variable price will be used.items, price = [&quot;groceries&quot;, &quot;clothing&quot;, &quot;insurance&quot;], 228.65str_format1 = &quot;I paid for &quot; + str(items) + &quot; and it costs &quot; + str(price)str_format2 = &quot;I paid for %s and it costs %s&quot; % (items, price)str_format3 = &quot;I paid for {} and it costs {}&quot;.format(items, price)The newest method is f-strings. Personally, I find this the most readable method. An f-string starts with a f or F before the quotation marks. Variables are placed within the brackets directly in the string. It is also possible to place actual code within the brackets, such as calculations and function calls. Preferably variables are well defined before using them in strings, but its great there is the option.items, price = [&quot;groceries&quot;, &quot;clothing&quot;, &quot;insurance&quot;], 228.65str_format4 = f&quot;I paid for {items} and it costs {price}&quot;It is also possible to specify specific formatting similar to the old-style format method. After the variable name use a : followed by the necessary flags for the formatting. A value can also be directly supplied.Some common flags are for formatting decimals, using .&amp;lt;number of digits&amp;gt;f, or getting the variable name with the associated value with = (Note that there is no :). Scientific notation can be done using the flag e.It is also possible to center, left align, or right align using ^, &amp;lt;, and &amp;gt; respectively followed by the amount of space it needs to be justified with. This can be combined with symbols as seen below. Another common use is formatting datetime objects.value = 1234.123456print(f&quot;Some value {value:.2f}&quot;) # Some value 1234.12print(f&quot;Some value {value:.4f}&quot;) # Some value 1234.1234print(f&quot;Some {value=}&quot;) # Some value=1234.123456print(f&quot;Some value {value:e}&quot;) #Some value 1.234123e+03value = &quot;abcd&quot;print(f&quot;|{value:^10}|&quot;) # | abcd |print(f&quot;|{value:-&amp;lt;10}|&quot;) # |abcd------|print(f&quot;|{value:=&amp;gt;10}|&quot;) # |======abcd|import datetimetime = datetime.datetime.now()print(f&#39;{time:%Y-%m-%d %H:%M}&#39;) # 2022-11-25 15:39Of course, these are only some examples of the formatting that can be done with f-strings. There are many more options available just a quick google search away. At the very least I hope this gives a starting point for using f-strings and seeing the great use-cases with them." }, { "title": "The Importance of Graphic Design Skills", "url": "/posts/the-importance-of-graphic-design-skills/", "categories": "Personal", "tags": "Soft-Skills", "date": "2022-11-12 00:00:00 +0100", "snippet": "Though the main tasks of a bioinformatician are programming software for data-analysis, actually analyzing the data, and further research to connect the computational data and results with the biology, there is still the need to communicate. Though some of it can and will be done in writing, a single image says more than a thousand words if done right.Graphic design is the use of techniques to guide someone through a message in a clear and concise manner. Structuring images, but also text, in a way that draws attention and is readable is more difficult than one might assume.The amount of information, space, color, scale, direction are all factors that have to be considered when shaping the message. In addition, they need to be altered to fit the audience you are communicating towards. A patient will require a different form than a fellow researcher, even if the core message is the same.Just as writing is an important skill to learn, so is graphic design. Especially as it can support your writing, or even work on its own. It will help with your presentations, posters, as well as organizing your information.Great starting points for learning are Youtube, and online courses. But it also helps to simply actively think about the world and its messages around you. Think why a billboard or a poster is made in a that specific way. How does an advertisement grab your attention? Training yourself into understanding graphic design will at the very least help in seeing your own work in a similar light. In turn improving it." }, { "title": "Screen: A Powerful Tool", "url": "/posts/screen-a-powerful-tool/", "categories": "Software-Development, Tools", "tags": "Linux, BASH", "date": "2022-10-29 01:00:00 +0200", "snippet": "Screen is an amazing tool for linux that has saved me more than once, allowing me to run long-running processes in the background and disconnect from them. In turn allowing me to continue working without issue. Screen is already installed on most linux distributions, so it is easy to try out.To start, simply type the command screen. This opens an interactive background process. Your standard commands work here and will continue as long as the computer (or server) stays on. To get out of the screen use Ctrl + A + D. To completely stop the screen and thus also stop all processes in it, use Ctrl + D. Use the same command again to create a new screen.To reconnect to your last screen (background process) use the command screen -r. Do note that this will always connect to your last connected screen. To reconnect to different screens you will need to know the id of the screen in question.To get the ids of the different screens it is easiest to use the command screen -ls. This returns a list of your screens, including their ids as seen below. To reconnect to a screen use screen -r &amp;lt;id&amp;gt;. So in this case screen -r 180.tty1.DESKTOP-7OCE5L2.There is a screen on: 180.tty1.DESKTOP-7OCE5L2 (10/17/22 17:26:26) (Detached)1 Socket in /run/screen/S-root.These are basic commands, but it allows for executing lond-during pipelines and software without having to keep your own computer on all the time. It is best to use this command on servers as you know they will stay on, even if you walk away. With this tool you can also easily start something at the end of the work-day and come back in the morning for your results. Screen really can make live easier." }, { "title": "The Importance of Writing Skills", "url": "/posts/the-importance-of-writing-skills/", "categories": "Personal", "tags": "Soft-Skills", "date": "2022-10-15 01:00:00 +0200", "snippet": "The main tasks of a bioinformatician are programming software for data-analysis, actually analyzing the data, and further research to connect the computational data and results with the biology. However, as important is writing down the information for others (and yourself).As a bioinformatician, the writing will be mostly documentation for the created software. Thus manuals such that others can use the software, comments in the code for other developers, and technical documents describing the software design process.The pieces that are written from a research perspective are papers, posters, and similar documents. Here the focus lies on explaining the theoretical methods and the results gained from the performed analyses. Though it is great to have well designed and documented code, if it is not used there is no reason for the software to exists. Similarly, if the gained knowledge from the analysis is not preserved, why would one perform the analysis?However, is knowledge truly preserved when no one understands what is written down? This is were writing skills show their importance. Creating a comprehensive story and understanding the audience you are writing for are the basic writing skills to learn.Other writing skills include of course grammar and spelling. Though more important in my opinion is structure. How to organize the text such that it flows and connects. That it is ordered in a way that is expected. Many templates help with this, such as thr three- and seven-act structure for novels, or the standard setup of Introduction, Materials and Methods, Results, Discussion, and finally Conclusion seen in scientific papers.Understand these structures, so they can be altered to suit the actual situation and needs of the knowledge that is being written down. Understanding the different structures will also help in creating your own templates for documents and aid in organizing the information from the start.Though this is just the tip of the iceberg, these points are a great starting point to improve upon with regards to writing skills. Improving these skills will help in communicating better. It will likely also improve the general work of software development and research as well, as writing the information down forces one to think about it in more detail, while placing it in an overall picture. These two reasons shown again why writing skills are important to master." }, { "title": "Contingency Tables in R", "url": "/posts/contingency-tables-in-r/", "categories": "Software-Development, Programming", "tags": "R", "date": "2022-10-01 01:00:00 +0200", "snippet": "Contingency tables are a great tool in comparison analyses. In the simplest form a contingency table places one set of results against another set of results. In the example below two methods are compared. Each method produces two outcomes $A$ and $B$. Through the contingency table it is easily recognizable that the vast majority of the results between the methods match.| | | Method1 |||-----------|---|:---:|:---:|| | | A | B ||**Method2**| A | 20 | 1 || | B | 2 | 18 |Within R it is incredibly easy to make a contingency table. The built-in function table takes two vectors and makes… well, a table of it. The result is actually a contingency table. The first given vector is the columns, while the second given vector is the rows. If a category is not present it is not used in the table. Thus in the code example below the columns will have category $C$, but the rows will not.method1 &amp;lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;)method2 &amp;lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;, &quot;A&quot;)contingency &amp;lt;- table(method1, method2)The contingency table works for categorical data. A contingency table can be made from two lists with different categories, and the number of categories can be different between the lists. The three requirements for a contingency table are that 1) there are only two lists, 2) the lists are of the same length, and 3) that the lists are ordered in the same manner. In the example below a dataframe with only two columns is given as the input. If there are more columns an error will be given. In the case of a dataframe the names of the columns (outcome and medication) are also added to the output.dataframe &amp;lt;- data.frame(outcome = c(&quot;Good&quot;, &quot;Good&quot;, &quot;Bad&quot;, &quot;Bad&quot;, &quot;Bad&quot;), medication = c(&quot;X&quot;, &quot;X&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;Y&quot;))contingency &amp;lt;- table(dataframe$outcome, dataframe$medication)The contingency table is quite simple to implement in R. It can be extended by adding row and column totals, calculating metrics from it, or by generalizing the code into a function. Of course there are also packages that make nice contingency tables, some even make nice graphics of it using mosaicplots." }, { "title": "Fasta and Fastq File Formats", "url": "/posts/fasta-and-fastq-file-formats/", "categories": "Bioinformatics", "tags": "Bioinformatics", "date": "2022-09-17 01:00:00 +0200", "snippet": "Nucleotide sequences (DNA or RNA) are one of the most common data types to work with in bioinformatics. Depending on the use-case they are stored in a fasta or fastq formatted file.Fasta is a simple format to record data. A record consists of two lines. The first line starts with the symbol &amp;gt;. This is followed by… basically anything. Usually different identifiers are given which are separated by |, but it could just as well be a place name such as Los Angeles. On the next line is the sequence of data that needs to be stored. This structure is continued for all the data. An example can be seen below.&amp;gt; Identifier_1|Identifier_2|Identifier_3AATTCGCGCGGATGCGCTATCAAACG&amp;gt; Gene Name|Ensembl IDGATAATTCGCGCGGATGCAAACGAATTCGCGCGGCGCTATCAAACGGCTATCThe fastq format uses records of four lines. The first is similar to the first line in the fasta format. It starts with @ and is followed by an identifier, as well as other information if needed. The second line is the actual sequence again. The third line starts with +. This can be followed by the same identifier and information on the first line. The identifier and information on the third line is optional. The fourth and final line in a record is a string of characters representing the quality of the bases in the second line according to the Phred scoring algorithm. Naturally this line has the same length as the second one.@SequenceID GATAATTCGCGCGGATGCAAACGAATTCGCGCGGCGCTATCAAACGGCTATC+SequenceID!&#39;&#39;*(((***+))%%%++)(%%%..1***-+*&#39;&#39;))**55CF&amp;gt;&amp;gt;&amp;gt;&amp;gt;CCCC65In a sense, the fastq format is an extension of the fasta format. Though fastq is useful for your own sequencing data and raw data for sequence analysis, fasta formatted files still have an important function. Fasta formatted files are great for reference sequences, such as genomes and canonical gene sequences. These do not need quality scores as they are usually aggregates of multiple data sources to get the highest quality reference possible at the time.Thus the fasta and fastq are similar, with some key differences, which helps in their own use-cases. Both are important in bio-informatics with your own data often in the fastq format, while references are usually in the fasta format." }, { "title": "Phred Scores", "url": "/posts/phred-scores/", "categories": "Bioinformatics", "tags": "Bioinformatics", "date": "2022-09-03 01:00:00 +0200", "snippet": "Phred scores are a common metric in bioinformatics to assign quality, in most cases to individual bases (A, T, C, G). Though they are used as quality scores, Phred scores actually represent a probability. A lower Phred score indicates a lower probability.In many cases, including the quality of sequenced nucleotides, the probability tells the certainty that the given result is correct. In the case of the nucleotides it gives the certainty that the nucleotide is correctly sequenced. The lower Phred scores represent a lower certainty, and in turn a lower quality. This same concept can be applied to sequence alignment quality and the determination of consensus sequences. Phred scores are rarely seen outside of bioinformatics, if at all.Phred scores are calculated as the base 10 logarithm ($\\log_{10}$) of a given probability that the value is incorrect ($P$). The Phred score is made positive and scaled by multiplying with $-10$. This gives the following formula:     $Q = -10 \\log_{10}P$Thus a probability of 0.1 (10%, 1 in 10) will give us a Phred score ($Q$) of 10. Whereas a probability of 0.01 (1%, 1 in 100) will give a $Q$ score of 20. The pattern continues with each ten-fold decrease in the probability being a 10 point increase in the Phred scores. Of course it is also possible to go from a Phred score to a probability using the formula below:     $P = 10 \\dfrac{-Q}{10}$Though we can have small differences explained with normal numbers now, the real smart thing about Phred is the encoding that con now be done. The Phred scores can be encoded into single characters by using ASCII encoding. To ensure that the characters are readable and visible (no whitespace charatcers) the value 33 is added to the Phred score. Thus $Q=$ 10 becomes the character +. With this we can have values and quality scores matching in two lines beneath each other, saving a lot of space in comparison to writing the value 99.9% each time." }, { "title": "Multiple Conditions Testing in Python", "url": "/posts/multiple-conditions-testing-in-python/", "categories": "Software-Development, Programming", "tags": "Python", "date": "2022-08-20 01:00:00 +0200", "snippet": "Simple conditions are easily taken care of with if-else statements. In cases that multiple conditions need to be satisfied the statements are extended using an AND operator. This can lead to complex, long, and messy statements as seen in an example below.a = 100b = 10c = [1, 2, 3, 4, 5]if a &amp;gt;= 90 and b &amp;lt;= 50 and len(c) == 5: print(&quot;all true&quot;)else: print(&quot;all false&quot;)In Python these statements can be made clearer with the all keyword. Simply put your conditions in a list and call them in the if branch with the keyword all. This way it is immediately clear that all conditions are satisfied. And with proper formatting it is easier to read and modify the different conditions.a = 100b = 10c = [1, 2, 3, 4, 5]conditions = [a &amp;gt;= 90, b &amp;lt;= 50, len(c) == 5]if all(conditions): print(&quot;all true&quot;)else: print(&quot;all false&quot;)Where just a single condition out of multiple needs to be satisfied we use an OR operator. Similarly with AND this can lead to difficult to read statements, especially later on when you have to revisit your own code.a = 100b = 10c = [1, 2, 3, 4, 5]if a &amp;gt;= 90 or b &amp;lt;= 50 or len(c) &amp;lt; 5: print(&quot;at least one true&quot;)else: print(&quot;all false&quot;)In Python these statements can be made clearer with the any keyword. This follows again the same logic as above. In this case the conditions are chained with the OR operator. Note that with OR only a single condition needs to evaluate as true.a = 100b = 10c = [1, 2, 3, 4, 5]conditions = [a &amp;gt;= 90, b &amp;lt;= 50, len(c) &amp;lt; 5]if any(conditions): print(&quot;at least one true&quot;)else: print(&quot;all false&quot;)The lists can be larger than 3 conditions. The larger the number, the more useful the methods all and any will be. However, in the case you have 2 conditions (or 3 really simple ones) using an if statement in combination with and/or will likely be the better option. Readability matters in the end, and this will depend on the number and complexity of the conditions." }, { "title": "The Efficiency in Inefficiency", "url": "/posts/the-efficiency-of-inefficiency/", "categories": "Personal", "tags": "Other", "date": "2022-08-06 01:00:00 +0200", "snippet": "A great goal in business, programming, and perhaps even life is efficiency. An enormous amount of effort is put into processes and automation to perform as efficiently as possible. This can be seen in factories, shops, and the many processes in a company. It seems as if efficiency is the highest good, and inefficiency is the greatest evil. However, there is a need for inefficiency. Inefficiency can be a force of good.Inefficiency is able to be as good as efficiency can be bad. A maximum efficiency in business activities would entail that if even one thing changes the whole house of cards falls down. A simple (and usually positive) change as an increased number of orders for the company cannot be handled as the company is already at maximum efficiency and thus indirectly maximum capacity.Another example would be when a supply line can only function when everything works without any delay or any defects. It became clear during the covid-19 pandemic (2020-2022) that the worlds supply chain is highly efficient, but also extremely brittle. When cases of covid-19 infections became too high in China, they announced a nation-wide quarantine to curb the spread of the virus. This in turn had major effects on the supply chains, creating massive shortages (mainly electronics related) in the world.Only a single change, large or small, was able to create impactful repercussions. Simple solutions for these are making the overall system inefficient, but also robust and scalable. Introducing redundancy allows for parts to fall away while the rest can continue to function. Larger timelines allows for delays and mistakes, basically underpromise and overdeliver on the time aspect with this solution.Of course, this does not simply apply to only organizations. I personally like to be efficient with my resources. These resources include money, time, and quality. But I do not, and more importantly should not, focus on optimizing every little thing in my life. If I did that I would likely go mad. The optimization alone would consist of an incredible amount of variables such that I would never be able to act upon the optimized choices.Thus while efficiency is great, I hope that I’ve shown there is a need for some inefficiency in the system. To increase the robustness of the system or to keep yourself from going mad. It is fine to not be optimal a full 100% of the time. It is good to let yourself waste some time or money once in a while." }, { "title": "In-place Variable Swapping in Python", "url": "/posts/in-place-variable-swapping-in-python/", "categories": "Software-Development, Programming", "tags": "Python", "date": "2022-07-23 01:00:00 +0200", "snippet": "In many algorithms, such as bubble sort, there is a need to swap values. In most cases this happens in the same array or list object. Others require the swapping of variables. A simple solution is through the use of a temporary variable that safeguards the original value.a = 1b = 0 temp_a = a # 1a = b # 0b = temp_a # 1In Python it is easier to swap variables. There is no need to use a temporary variable. You can simply swap them by reordering them as in the following example. This simple reassignment works also for more than two variables.a = 1b = 0a, b = b, a # 0, 1In all cases the order is what matters with the reassignment. This same logic can be used for the assignment of values. Or the reordering of multiple variables. This works regardless of the number of variables. Naturally it is also possible to mix variable and valuesa, b, c, d = 0, 1, 2, 3a, b, c, d = d, c, b, a # 3, 2, 1, 0a, b, c, d = d, c, 5, a # 3, 2, 5, 0" }, { "title": "Short Booleans in R", "url": "/posts/short-booleans-in-r/", "categories": "Software-Development, Programming", "tags": "R", "date": "2022-07-09 02:00:00 +0200", "snippet": "Booleans are the logic operators representing true and false in programming languages. It could also be seen as a 0 or 1. In R the keywords TRUE and FALSE are used. Please note that these are fully capitalized, otherwise R will not recognize the booleans.if (TRUE == TRUE) {}if (FALSE == FALSE) {}In R you also have the option to use a shortened version of the booleans. These are a capitalized T and F, as seen below.if (T == T) {}if (F == F) {}You can also mix between the standard and short notation of booleans in our statements.if (TRUE == T) {}if (F == FALSE) {}I advise to choose between the standard or short notation and consistently apply it in your own projects. Use the standard notation in team projects where not everyone is familiar with the R programming language." }, { "title": "Ellipses instead of Pass keyword", "url": "/posts/ellipses-instead-of-pass-keyword/", "categories": "Software-Development, Programming", "tags": "Python", "date": "2022-06-25 01:00:00 +0200", "snippet": "During software development it is common to declare functions and classes to implement later. To prevent crashes we usually use a statement that prevents the function from loading or a statement that does “nothing” and thus lets the program continue. In Python the keyword pass is usually used for this purpose. It basically skips the block of code and continues on with the program. Running the below snippet will do nothing.def some_function(): passsome_function()In a sense, the meaning behind the keyword pass will depend on the context. Often it will indicate that the actual code still needs to be implemented. However this is not necessarily the case as pass can be used functionally. This could cause confusion in larger projects.A clearer way to denote that code still needs to be implemented in Python is with ellipses. Ellipses are the three little dots (...) that indicate the omission of text in conventional writing. This way would be a lot safer if the entire team is familiar with Python and give more functional context to pass when used. Personally, I favor the readability of ... over pass. It makes the purpose of the statement clear without disambiguating between function and communication, while doing the exact same thing.def some_function(): ...some_function()" }, { "title": "Starting a Blog", "url": "/posts/Intro/", "categories": "Personal", "tags": "Other", "date": "2022-06-11 01:00:00 +0200", "snippet": "Hello World!Corny rehashed jokes aside, welcome to this small little corner of the internet.If you read this I will likely already have made several posts on the blog and you simply stumbled upon this first one.This first post is to set some expectations and accountability for myself.My Goals for the BlogI want to create this blog to share my findings regarding… well basically anything I find interesting. As a result posts will mostly be about software development, programming, and bioinformatics. However, I could also easily write about personal values, psychology, physics, art, or gaming.The plan is to write and post every other week. The length will of course differ by the topic, but I will try to be clear and concise. In a way this blog is also an exercise for me to improve my writing skills, while allowing me to focus on the topics that interest me. Hopefully this blog can one day become a repository of my learnings, my interests, and my career." } ]
